import threading
import time
import json
from flask import Flask, jsonify
import psycopg2
import requests


# SELECT
#     gs.id, gs.name
# FROM
#     public.geoentity_source AS gs
# LEFT JOIN
#     public.geoentity_pyramid_levels AS gpl
# ON
#     gs.id = gpl.geoentity_source_id
# WHERE
#     gpl.geoentity_source_id IS NULL;

app = Flask(__name__)

# Database configuration
DATABASE_CONFIG = {
    "dbname": "geoentity_stats",
    "user": "postgres",
    "password": "Vedas@123",
    "host": "192.168.2.149",
    "port": "5433"
}

# External API endpoint
SOURCE_API_URL = "https://vedas.sac.gov.in/geoentity-services/api/geoentity-sources/"

# File to store cached data
CACHE_FILE = "cached_data.json"
CACHE_TIMEOUT_SECONDS = 5 * 60 # 5 minutes

def update_cache():
    """
    Fetches data from the API and DB, and saves it to a JSON file.
    """
    print("Starting cache update...")
    try:
        # Step 1: Fetch source data from external API
        response = requests.get(SOURCE_API_URL)
        response.raise_for_status()
        api_data = response.json().get("data", [])

        # Step 2: Get a list of IDs to check
        source_ids = [source.get("id") for source in api_data if source.get("id") is not None]

        if not source_ids:
            print("No source IDs found from API.")
            return

        # Step 3: Query the database
        conn = psycopg2.connect(**DATABASE_CONFIG)
        cur = conn.cursor()
        sql_query = """
            SELECT
                gs.id,
                CASE
                    WHEN EXISTS (SELECT 1 FROM public.geoentity_pyramid_levels gpl WHERE gpl.geoentity_source_id = gs.id)
                    THEN 'yes'
                    ELSE 'no'
                END AS pyramid_levels_available
            FROM
                public.geoentity_source gs
            WHERE
                gs.id = ANY(%s);
        """
        cur.execute(sql_query, (source_ids,))
        db_results = cur.fetchall()
        cur.close()
        conn.close()

        # Step 4: Combine data
        db_map = {row[0]: row[1] for row in db_results}
        final_results = []
        for source in api_data:
            source_id = source.get("id")
            source_name = source.get("name")
            if source_id is not None and source_name is not None:
                final_results.append({
                    "id": source_id,
                    "name": source_name,
                    "pyramid_levels_available": db_map.get(source_id, 'no')
                })

        # Step 5: Save to JSON file
        with open(CACHE_FILE, "w") as f:
            json.dump(final_results, f)
        print("Cache updated successfully.")

    except (requests.exceptions.RequestException, psycopg2.DatabaseError, Exception) as e:
        print(f"Error during cache update: {e}")

def start_cache_thread():
    """
    Starts a background thread to update the cache periodically.
    """
    def run_job():
        while True:
            update_cache()
            time.sleep(CACHE_TIMEOUT_SECONDS)
    
    # Run the initial update immediately
    update_cache()
    
    # Start the background thread
    thread = threading.Thread(target=run_job, daemon=True)
    thread.start()

@app.route("/check-pyramid-levels", methods=["GET"])
def check_pyramid_levels():
    """
    Serves the pre-cached data from the JSON file.
    """
    try:
        with open(CACHE_FILE, "r") as f:
            cached_data = json.load(f)
        return jsonify(cached_data)
    except FileNotFoundError:
        return jsonify({"message": "Cache not yet available, please wait."}), 503
    except Exception as e:
        return jsonify({"error": f"Error reading cached data: {e}"}), 500

if __name__ == "__main__":
    start_cache_thread()
    app.run(host="0.0.0.0", debug=True)
